{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Phrase_based_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hbasgol/ibm_models/blob/master/phrase_based_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf19rsUwSGIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "###################### modules/libraries #######################################\n",
        "# import os -> to see the path of folders and libraries\n",
        "# from google.colab import drive -> to connect google drive\n",
        "# import codecs -> to save Turkish characters without problem\n",
        "# import json -> to save dictionaries structured for representing tables as\n",
        "#   .json documents, json.dump is used for writing and json.loads is used for\n",
        "#   reading .json strings as dictionaries\n",
        "# from itertools import product\n",
        "# from itertools import permutations -> to find possible alignments between\n",
        "#   English and Turkish phrases that structure corresponding sentences\n",
        "# import numpy as np -> for simple mathematical operations such as taking absolute\n",
        "#   of a number with np.abs or summing values in a list with np.sum\n",
        "# import unicodedata -> for removing Turkish characters in the corpus\n",
        "# from collections import Counter -> to count elements in a list, it used for\n",
        "#   normalization\n",
        "################################################################################\n",
        "import os\n",
        "from google.colab import drive\n",
        "import codecs\n",
        "import json\n",
        "from itertools import permutations\n",
        "from itertools import product\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "from collections import Counter\n",
        "################################################################################\n",
        "\n",
        "\n",
        "###################### connectcolab ############################################\n",
        "# the function connectcolab is used to receive Google Drive documents and\n",
        "#   determine the path the files are written to or read from.\n",
        "################################################################################\n",
        "def connectcolab():\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "path=\"/content/drive/My Drive/Colab Notebooks/Machine Translation/IBM2-tables/\"\n",
        "################################################################################\n",
        "\n",
        "\n",
        "###################### tokenization ############################################\n",
        "# takes sentencelist such as [[\"first sentence\"], [\"second sentence\"], [\"third sentence\"], ...]\n",
        "# and op2, which is an operator to deteck whether the given sentence list\n",
        "# is composed of Turkish sentences or English sentences\n",
        "#\n",
        "# -> returns [[\"first\", \"sentence\"], [\"second\", \"sentence\"], [\"third\", \"sentence\"], ...]\n",
        "#\n",
        "# for the inconsistency of the corpus of Turkish sentences, Turkish characters \n",
        "# such as \"ş, ğ, ç\" etc. have been turned into s, g and c.\n",
        "#\n",
        "# this function is different than those of others implemented for IBM 1 and IBM 2\n",
        "# because this does not involve NULL token, which is not important for\n",
        "# phrase-based translation\n",
        "################################################################################\n",
        "\n",
        "def tokenization(sentencelist, op2):\n",
        "  if op2 == \"t\":\n",
        "    return [[*map(rm_turkish, i.split(\" \"))] for i in sentencelist]\n",
        "  if op2 == \"e\":\n",
        "    return [i.split(\" \") for i in sentencelist]\n",
        "    \n",
        "################################################################################\n",
        "\n",
        "\n",
        "#################### rm_turkish ################################################\n",
        "# the function takes a word that is a string data type and change Turkish\n",
        "# characters into English counterparts.\n",
        "#\n",
        "# takes uçuyorum -> returns ucuyorum\n",
        "# şenlik -> senlik\n",
        "#\n",
        "# to remove the ambiguity of the corpus in terms of Turkish sentences\n",
        "#\n",
        "# the function is used in another function named tokenization\n",
        "################################################################################\n",
        "\n",
        "def rm_turkish(word):\n",
        "  normalized = unicodedata.normalize('NFD', word)\n",
        "  word = \"\".join([c for c in normalized if not unicodedata.combining(c)])\n",
        "  return word\n",
        "\n",
        "################################################################################\n",
        "\n",
        "\n",
        "############################### get_sample #####################################\n",
        "# the functiont takes one argument, n_sample, which determines the number of \n",
        "#   sentences which will be using in the function. The n_sample of sentences\n",
        "#   are then separated into test and train sets with a ratio of 1/9\n",
        "#\n",
        "# the function returns english_train, turkish_train, english_test, turkish_test, \n",
        "#   english_train: english sentences the model to be trained as a list of list\n",
        "#   turkish_train: turkish sentences the model to be trained as a list of list\n",
        "#   english_test: english sentences the model to be tested as a list of list\n",
        "#   english_test: english sentences the model to be tested as a list of list\n",
        "#\n",
        "# the difference between this function and those of others implemented for\n",
        "#   IBM 1 and IBM 2 is that it does not find the unique words in the corpus.\n",
        "#   Because, phrase-based translation does not need them.\n",
        "#\n",
        "# the function named connectcolab() used in this function to connect Google Drive\n",
        "################################################################################\n",
        "\n",
        "def get_sample(sample):\n",
        "  connectcolab()\n",
        "    \n",
        "  english_list = []\n",
        "  with open(\"/content/drive/My Drive/Colab Notebooks/Machine Translation/corpus/english.txt\", \"r\") as english:\n",
        "    for cnt, line in enumerate(english):\n",
        "      english_list.append(line.rstrip())\n",
        "\n",
        "  turkish_list = []\n",
        "  with open(\"/content/drive/My Drive/Colab Notebooks/Machine Translation/corpus/turkish.txt\", \"r\") as turkish:\n",
        "    for cnt, line in enumerate(turkish):\n",
        "      turkish_list.append(line.rstrip())\n",
        "  \n",
        "  english_list=english_list[:sample]\n",
        "  turkish_list=turkish_list[:sample]\n",
        "  \n",
        "  rtrain = int(sample*0.9)\n",
        "\n",
        "  english_list = tokenization(english_list, \"e\")\n",
        "  turkish_list = tokenization(turkish_list, \"t\")\n",
        "  english_train, turkish_train = english_list[:rtrain], turkish_list[:rtrain]\n",
        "  english_test, turkish_test = english_list[rtrain:], turkish_list[rtrain:]\n",
        "  del english_list\n",
        "  del turkish_list\n",
        "  print(\"sentences for training have been processed\")\n",
        "  return english_train, turkish_train, english_test, turkish_test\n",
        "\n",
        "################################################################################\n",
        "\n",
        "\n",
        "########################## save/read_t_table functions #########################\n",
        "# save_phrases takes a number of phrase kept in a nested dictionary and\n",
        "#   saves the dictionary as a .json format with the help of op argument\n",
        "#   op argument determines the direction of phrases (English to Turkish)\n",
        "#   or Turkish to English\n",
        "#\n",
        "# read_phrases takes an argument named op to receive the .json file involving\n",
        "#   phrases as a nested dictionary. The function returns phrases as a nested\n",
        "#   dictionary format\n",
        "#\n",
        "# save_language_model takes language_model and two operators named op and op1\n",
        "#   language_model is a dictionary involving n-grams and their probabilities\n",
        "#   op is given to determine the language, Turkish or English\n",
        "#   op1 is given to determine the n-gram, whether bi-gram or three-gram\n",
        "#   the function saves language model with a .json file format\n",
        "#\n",
        "# read_language_model takes two operators: op and op1. The first one is given\n",
        "#   to determine the language and the second one is for determining the\n",
        "#   type of n-gram, whether it is three-gram or bi-gram, or other types.\n",
        "#   the function returns the corresponding language model as nested dictionary\n",
        "#   format\n",
        "#\n",
        "# read_viterbi_alignments takes one argument: op, which determines the direction\n",
        "#   of alignment, whether English to Turkish or Turkish to English and returns\n",
        "#   viterbi alignments as a nested dictionary\n",
        "#   Since .json data format does not support integers to be dictionary key,\n",
        "#   after reading the nested dictionary, all keys that have been saved as a\n",
        "#   string by the json module are turned into corresponding integers\n",
        "#     example: {1: {0: 1, 1: 2, 2, 4}, 2: {0:1, 2:2, 4:6 ...} ...}\n",
        "#     in which the first key is sentence index and inner key represents\n",
        "#     e ---> f or f ---> e, {0: 1, 1: 2}\n",
        "################################################################################\n",
        "\n",
        "def save_phrases(phrases, op):\n",
        "  with codecs.open(path+\"phrases-\"+op+\".json\", 'w', 'utf-8') as f:\n",
        "    json.dump(obj=phrases, fp=f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "def read_phrases(op):\n",
        "  with codecs.open(path+\"phrases-\"+op+\".json\") as f:\n",
        "    return json.loads(f.read())\n",
        "    \n",
        "def save_language_model(language_model, op, op1):\n",
        "  with codecs.open(path+\"language_model-\"+op+\"-\"+op1+\".json\", 'w', 'utf-8') as f:\n",
        "    json.dump(obj=language_model, fp=f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "def read_language_model(op, op1):\n",
        "  with codecs.open(path+\"language_model-\"+op+\"-\"+op1+\".json\") as f:\n",
        "    return json.loads(f.read())\n",
        "  \n",
        "def read_viterbi_alignments(op):\n",
        "  with codecs.open(path+\"IBM2-viterbi\"+op+\".json\") as f:\n",
        "    alignments = json.loads(f.read())\n",
        "    alignments_int = {int(i): {int(j): alignments[i][j] \\\n",
        "                               for j in alignments[i]}  \\\n",
        "                               for i in alignments}\n",
        "  del alignments\n",
        "  return alignments_int\n",
        "\n",
        "################################################################################\n",
        "\n",
        "\n",
        "############################## v_to_tuple ######################################\n",
        "# The function takes a viterbi alignment (alignments_int) that can be in both\n",
        "#   direction and returns a dictionary involving alignments as tuples such that\n",
        "#   {1: {(0, 1), (1, 2), (2, 4), ...}, 2: {...}...}\n",
        "#   {j: {(e1, f1), (e2, f2), (e3, f3), (e4, f5)}}, or vice versa\n",
        "#   Since intersection and union operations are required to find possible phrases\n",
        "#   the data structure of the viterbi alignments have been changed.\n",
        "################################################################################\n",
        "\n",
        "def v_to_tuple(alignments_int):\n",
        "  vt2e_ps={}\n",
        "  for key, val in alignments_int.items():\n",
        "    vt2e_ps[key]=[]\n",
        "    for key_a, val_a in val.items():\n",
        "      vt2e_ps[key].append((key_a, val_a))\n",
        "  return vt2e_ps\n",
        "\n",
        "################################################################################\n",
        "\n",
        "\n",
        "############################ alignment functions ###############################\n",
        "# The functions are written according to pseudocodes that were given in\n",
        "#   Statistical Machine Translation, written by Philipp Koehn, which is used\n",
        "#   textbook of the course\n",
        "#\n",
        "# grow_diag_final function consists of two functions that are grow_diag and final.\n",
        "#   The function takes viterbi alignments as vt2e and ve2t and aligned sentences\n",
        "#   in the corpus as e_sentence as English sentence and f_sentence as Turkish\n",
        "#   sentence, or vice versa, for bi-direction translation\n",
        "#   The function returns alignment that will be used in phrase extraction and\n",
        "#   intersect. \n",
        "#   alignment involves a set of tuples referring to alignments between sentences\n",
        "#   such that alignment = set((1, 2), (2, 2), (3, 1), (1, 4))\n",
        "#\n",
        "# grow diag function takes neighboring points as tuples,\n",
        "#   alignment which is the intersect of viterbi alignments,\n",
        "#   viterbi alignments as vt2e and ve2t,\n",
        "#   and two aligned sentences as e_sentence for English and f_sentence for Turkish\n",
        "#   The function searches for neighboring alignments and add the alignment if it\n",
        "#   is eligible\n",
        "#\n",
        "# final function takes viterbi alignments as vt2e and ve2t\n",
        "#   alignment points taken from grow_diag as alignment\n",
        "#   two aligned sentences as e_sentence and f_sentence\n",
        "#   and returns possible alignments by adding new points\n",
        "################################################################################\n",
        "\n",
        "def grow_diag_final(vt2e, ve2t, e_sentence, f_sentence):\n",
        "  neighboring = {(-1, 0), (0, -1), (1, 0), (0, 1),\n",
        "                   (-1, -1), (-1, 1), (1, -1), (1, 1)} # neighbors, move or swap\n",
        "  vt2e, ve2t = set([(x+1, y) for x, y in ve2t if y!=0]), set([(y, x+1) for x, y in vt2e if y != 0]) # removing impact of null tokens\n",
        "  alignment = ve2t.intersection(vt2e)\n",
        "  intersect = ve2t.intersection(vt2e)\n",
        "  alignment = grow_diag(neighboring, alignment, vt2e, ve2t, e_sentence, f_sentence)\n",
        "  alignment = final(vt2e, ve2t, alignment, e_sentence, f_sentence) #--> there might be a problem here, because gets all sentence\n",
        "  return alignment, intersect\n",
        "  \n",
        "def grow_diag(neighboring, alignment, vt2e, ve2t, e_sentence, f_sentence):\n",
        "  len_a=len(alignment)\n",
        "  while True:\n",
        "    for e_word in range(1, len(e_sentence)+1):\n",
        "      for f_word in range(1, len(f_sentence)+1):\n",
        "        if (e_word, f_word) in alignment:\n",
        "          for e_new, f_new in [(e_word+x, f_word+y) for x, y in neighboring]:\n",
        "            # conditions for adding alignment point\n",
        "            if (not e_new in [i[0] for i in alignment] or\n",
        "              not f_new in [i[1] for i in alignment]) and \\\n",
        "              ((e_new, f_new) in ve2t.union(vt2e)):\n",
        "                alignment.add((e_new, f_new))\n",
        "    if len_a==len(alignment): \n",
        "      return alignment # stop condition\n",
        "    else:\n",
        "      len_a=len(alignment)\n",
        "      \n",
        "def final(vt2e, ve2t, alignment, e_sentence, f_sentence):\n",
        "  for e_word in range(len(e_sentence)):\n",
        "    for f_word in range(len(f_sentence)):\n",
        "      if (not e_word in [i[0] for i in alignment] or \\\n",
        "          not f_word in [i[1] for i in alignment]) and \\\n",
        "          (e_word, f_word) in ve2t.union(vt2e):\n",
        "          alignment.add((e_word, f_word))\n",
        "  return alignment\n",
        "\n",
        "################################################################################\n",
        "\n",
        "\n",
        "####################### phrase extraction functions ############################\n",
        "# The functions are written for extracting phrases with the help of alignments\n",
        "#   found. \n",
        "# phrase_extraction takes f_sentence, e_sentence and alignment and returns\n",
        "#   possible phrases in a list named BP.\n",
        "# extract is an helper function in phrase_extraction that takes specific starting\n",
        "#   and ending points in f_sentence and e_sentence. If the phrase to be extracted\n",
        "#   is eligible, then the function returns phrases\n",
        "################################################################################\n",
        "\n",
        "def phrase_extraction(e_sentence, f_sentence, alignment):\n",
        "  BP = []\n",
        "  for e_start in range(1, len(e_sentence)+1): \n",
        "    for e_end in range(e_start, len(e_sentence)+1):\n",
        "      f_start, f_end = (len(f_sentence), 0)\n",
        "      for (e, f) in alignment:\n",
        "        if e_start <= e <= e_end:\n",
        "          f_start = min(f, f_start)\n",
        "          f_end = max(f, f_end)\n",
        "      extracted_phrases = extract(f_start, f_end, e_start, e_end, \\\n",
        "                                  f_sentence, e_sentence, alignment)\n",
        "      for phrase in extracted_phrases:\n",
        "        BP.append(phrase)\n",
        "  return BP\n",
        "\n",
        "def extract(f_start, f_end, e_start, e_end, f_sentence, e_sentence, alignment):\n",
        "  if f_end == 0:\n",
        "      return []\n",
        "  for (e, f) in alignment:\n",
        "      if (e < e_start or e > e_end) and (f_start <= f <= f_end):\n",
        "          return []\n",
        "  E = []\n",
        "  f_s = f_start\n",
        "  while True:\n",
        "      f_e = f_end\n",
        "      while True:\n",
        "          e_phrase = (e_start, e_end)\n",
        "          f_phrase = (f_s, f_e)\n",
        "          E.append((e_phrase, f_phrase))\n",
        "          f_e += 1\n",
        "          if f_e in list(zip(*alignment))[1] or f_e > len(f_sentence):\n",
        "              break\n",
        "      f_s -= 1\n",
        "      if f_s in list(zip(*alignment))[1] or f_s < 1:\n",
        "          break\n",
        "  return E\n",
        "  \n",
        "################################################################################\n",
        "\n",
        "\n",
        "####################### phrase_alignment #######################################\n",
        "# The function takes five arguments:\n",
        "#   english_train involves English sentences as a list\n",
        "#   turkish_train involves Turkish sentences as a list\n",
        "#   vt2e and ve2t are viterbi alignments with two directions\n",
        "#   limit is an integer that determines the length of phrases to be extracted\n",
        "#   op is for determining the direction of phrase alignments for saving\n",
        "# The function returns a nested dictionary named ph_dict involving phrase translation\n",
        "#   probabilities that have been normalized.\n",
        "################################################################################\n",
        "\n",
        "def phrase_alignment(english_train, turkish_train, vt2e, ve2t, limit, op):\n",
        "  ph_dict={}\n",
        "  for index_e, e_sentence in enumerate(english_train):\n",
        "    f_sentence=turkish_train[index_e]\n",
        "    t2e = vt2e[index_e]\n",
        "    e2t = ve2t[index_e]\n",
        "    alignment, intersect = grow_diag_final(t2e, e2t, e_sentence, f_sentence)\n",
        "    for f_phrase, e_phrase in phrase_extraction(e_sentence, f_sentence, alignment):\n",
        "      e_start, e_end = f_phrase\n",
        "      f_start, f_end = e_phrase\n",
        "      if (e_end-e_start)<limit and (f_end-f_start)<limit:\n",
        "        if e_start == e_end:\n",
        "          ep = str(e_sentence[e_start-1])\n",
        "        else:\n",
        "          ep = \" \".join(e_sentence[e_start-1:e_end])\n",
        "          ep2 = \" \".join(e_sentence[e_start-1:e_end+1])\n",
        "        if f_start == f_end:\n",
        "          fp = str(f_sentence[f_start-1])\n",
        "        else:\n",
        "          fp = \" \".join(f_sentence[f_start-1:f_end])\n",
        "          fp2 = \" \".join(f_sentence[f_start-1:f_end+1])\n",
        "          \n",
        "        if fp not in ph_dict:\n",
        "          ph_dict[fp]={}\n",
        "        if ep not in ph_dict[fp]:\n",
        "          ph_dict[fp][ep]=0\n",
        "        ph_dict[fp][ep]+=1 #to easily count how many phrases that english phrase belongs to\n",
        "        \n",
        "        if fp2 not in ph_dict:\n",
        "          ph_dict[fp2]={}\n",
        "        if ep2 not in ph_dict[fp2]:\n",
        "          ph_dict[fp2][ep2]=0\n",
        "        ph_dict[fp2][ep2]+=1 #to easily count how many phrases that english phrase belongs to\n",
        "        \n",
        "  for key, val in ph_dict.items():\n",
        "    n=sum(list(val.values()))\n",
        "    for ep in val:\n",
        "      val[ep]/=n\n",
        "  save_phrases(ph_dict, op)\n",
        "  print(\"number of phrases: {}\".format(len(ph_dict.keys())))\n",
        "  print(\"phrase probabilities are found and saved!\")\n",
        "  return ph_dict\n",
        "\n",
        "################################################################################\n",
        "\n",
        "\n",
        "############################## db_reordering ###################################\n",
        "# The function takes two arguments: x and a\n",
        "#   x is for distortion\n",
        "#   a is for ratio that the distortion to be punished\n",
        "# The function returns a float number that the probability of translation\n",
        "#   will be multiplied with\n",
        "################################################################################\n",
        "\n",
        "def db_reordering(x, a): ## d function for re-ordering model\n",
        "  return a**np.abs(x)\n",
        "\n",
        "################################################################################\n",
        "\n",
        "def distance(f_sentence, e_sentence, fp, ep, corresp):\n",
        "  pi = corresp.index((fp, ep))\n",
        "  if pi == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    epi = corresp[pi]\n",
        "    eindex = e_sentence.index(epi[1])\n",
        "    ephrase = e_sentence[eindex-1]\n",
        "    t_index=0\n",
        "    for index, (fp, ep) in enumerate(corresp):\n",
        "      if ephrase == ep:\n",
        "        t_index=index\n",
        "    if pi < t_index+1:\n",
        "        corresps = corresp[pi+1:t_index]\n",
        "    else:\n",
        "        corresps = corresp[t_index+1:pi]\n",
        "    ys = [x for x, y in corresps]\n",
        "    return len(ys)\n",
        "  \n",
        "############################## language_model ##################################\n",
        "# The function takes four arguments: n, english, op and op1\n",
        "#   n is for determining the number of word in n-gram\n",
        "#   english is a list of sentence\n",
        "#   op and op1 is for writing en_lm which is a nested dictionary that involves\n",
        "#   n-gram probabilities\n",
        "#     op -> the language, English or Turkish\n",
        "#     op1 -> name of n-gram such as two-gram or three-gram\n",
        "# The function returns a nested dictionary that involves n-gram probabilities\n",
        "#   such that\n",
        "#   {...'\"So': {'how': {'have': {'you': 0.12312}}, \n",
        "#   'will': {'you': {'not': 0.435345}}},...} for 4-gram language model\n",
        "#   -> So how have you\n",
        "#   -> So will you not\n",
        "#\n",
        "# The function uses an helper function named normalization, which normalizes\n",
        "#   counts in the corpus\n",
        "#\n",
        "# Another helper function used in this function named nested_dict\n",
        "#   which gets an n_gram as a list such that [\"So\", \"how\", \"have\", \"you\"]\n",
        "#   and add elements of the list to the dictionary as keys\n",
        "################################################################################\n",
        "\n",
        "def language_model(n, english, op, op1): # n-gram model for language model in STM\n",
        "  st_symbol=\"<s>\" # starting symbol\n",
        "  en_lm={} # dictionary for counting\n",
        "  for index_e, e_sentence in enumerate(english):\n",
        "    e_sentence_n=e_sentence\n",
        "    for s in range(n-1):\n",
        "        e_sentence_n = [st_symbol]+e_sentence_n\n",
        "    for index_w, e_word in enumerate(e_sentence_n):\n",
        "      if e_word != st_symbol:\n",
        "        n_gram=e_sentence_n[index_w-n+1:index_w+1]\n",
        "        en_lm = nested_dict(n_gram, en_lm, en_lm) # nested dictionary creating with a recursive function\n",
        "        # such that it will be en_lm[w1][w2][w3] = p(w3|w1, w2) for 3-gram \n",
        "  en_lm=normalization(en_lm, n) # normalization\n",
        "  save_language_model(en_lm, op, op1)\n",
        "  return en_lm\n",
        "\n",
        "################################################################################\n",
        "\n",
        "\n",
        "############################### nested_dict ####################################\n",
        "# The function is a recursive function that takes three arguments: \n",
        "#   key, dic and dic_all \n",
        "#   key is an n-gram as [\"So\", \"how\", \"have\", \"you\"], which is 4-gram\n",
        "#   dic is the dictionary that n-grams will be added\n",
        "#   dic_all is the same dictionary as dic to return the dic as a whole.\n",
        "# The function returns a changed dictionary\n",
        "################################################################################\n",
        "\n",
        "def nested_dict(key, dic, dic_all):\n",
        "  if len(key)==1:\n",
        "    if key[0] in dic:\n",
        "      dic[key[0]]+=1\n",
        "    else:\n",
        "      dic[key[0]]=1\n",
        "  if len(key)==0:\n",
        "    return dic_all\n",
        "  else:\n",
        "    if key[0] not in dic:\n",
        "      dic[key[0]]={}\n",
        "    return nested_dict(key[1:], dic[key[0]], dic_all)\n",
        "  \n",
        "################################################################################\n",
        "\n",
        "\n",
        "############################### normalization ##################################\n",
        "# The function is a recursive function that takes four arguments\n",
        "#   dic is the dictionary involving n-gram counts\n",
        "#   n is the number of words in n-gram\n",
        "#   dic_all is a default argument that is used to copy dic\n",
        "#   c is a default argument that is used to count how many key is passed\n",
        "#\n",
        "# The function returns n-gram probabilities as a nested dictionary, which means\n",
        "#   that counts are turned into probabilities\n",
        "################################################################################\n",
        "\n",
        "def normalization(dic, n, dic_all=[], c=0):\n",
        "  if c == 0:\n",
        "    dic_all=dic\n",
        "  if (n-1) == c:\n",
        "    nm=np.sum(list(dic.values()))\n",
        "    for k, v in dic.items():\n",
        "      dic[k]/=nm\n",
        "  for key, val in dic.items():\n",
        "    if (n-1) != c:\n",
        "      normalization(dic[key], n, c=c+1)\n",
        "  return dic_all\n",
        "\n",
        "################################################################################\n",
        "\n",
        "\n",
        "############################### reach_n_gram ###################################\n",
        "# The function takes two arguments: key and dic\n",
        "#   key is a list involving n-grams such that [<s>, <s>, \"Who\"] as 3-gram\n",
        "#   dic is the dictionary involving n-grams\n",
        "# The fuction returns the probability of n-gram\n",
        "################################################################################\n",
        "\n",
        "def reach_n_gram(key, dic):\n",
        "  if type(dic)!=dict:    # if the function reaches the probability (float)\n",
        "    return dic\n",
        "  if not key[0] in dic:   # if the word could not found in N-gram, return 0 \n",
        "    return 0\n",
        "  return 0+reach_n_gram(key[1:], dic[key[0]])\n",
        "\n",
        "################################################################################\n",
        "  \n",
        "  \n",
        "############################### language_model_prob ############################\n",
        "# The function takes three arguments: e_sentence, n and lm_en\n",
        "#   e_sentence is an English sentence\n",
        "#   n is the number of word in n-gram\n",
        "#   lm_en is a nested dictionary involving n-grams and correspoding probabilities\n",
        "#\n",
        "# The function returns probability of language model for e_sentence\n",
        "#\n",
        "# The function firstly splits e_sentence into words and finds possible n-grams\n",
        "#   and probabilities of these n-grams are multiplied each other to find\n",
        "#   the probability of language model for e_sentence\n",
        "#\n",
        "# An helper function named reach_n_gram is used in this function, which returns\n",
        "#   probability of an n-gram by searching a nested dictionary, which is lm_en\n",
        "#   in this case\n",
        "################################################################################\n",
        "\n",
        "def language_model_prob(e_sentence, n, lm_en, info=False):\n",
        "  \n",
        "  # since the english sentence will be given manually,\n",
        "  # this does not affect argmax\n",
        "  # however, it is a valuable component of the standard model\n",
        "  # it can be used to assess the fluency of english sentences\n",
        "  \n",
        "  if type(e_sentence) == str:\n",
        "    e_sentence = [rm_turkish(e_sentence)]\n",
        " \n",
        "  st_symbol=\"<s>\"\n",
        "  ew_sentence=sum([w.split(\" \") for w in e_sentence], [])\n",
        "  e_sentence_n=ew_sentence\n",
        "  for s in range(n-1):\n",
        "    e_sentence_n = [st_symbol]+e_sentence_n\n",
        "  lm_p=1\n",
        "  for index_w, e_word in enumerate(e_sentence_n):\n",
        "    if e_word != st_symbol:\n",
        "      n_gram=e_sentence_n[index_w-n+1:index_w+1]\n",
        "      if info:\n",
        "        print(n_gram, reach_n_gram(n_gram, lm_en))\n",
        "      lm_p*=reach_n_gram(n_gram, lm_en) \n",
        "  return lm_p\n",
        "\n",
        "################################################################################\n",
        "\n",
        "\n",
        "############################### translation ####################################\n",
        "# The function implements the standard model of phrase-based translation\n",
        "# The function takes eight arguments:\n",
        "#   f_sentence and e_sentence are aligned sentences in the corpus\n",
        "#   they are structed as a combination of phrases in a list of list\n",
        "#   ph_dict is a nested dictionary involving phrase-translation probabilities\n",
        "#   n is the number of word in the n-gram, language model\n",
        "#   gram_en is a nested dictionary involving n-gram probabilities\n",
        "#   a is for distortion-reordering\n",
        "#\n",
        "# The function firsty finds language model probability for e_sentence\n",
        "#   if the language model probability is 0, then it returns a blank dictionary\n",
        "#   and the probability of f sentence to be translated as e_sentence as 0\n",
        "# If the function finds a value for probability of language model, it generates\n",
        "#   possible alignments between phrases.\n",
        "# It runs over possible alignments and for each alignment it creates a probability.\n",
        "#   by multiplying it with probability of language model.\n",
        "# After then, it returns all alignments (correspondances) with respective values\n",
        "#   referring to their probabilities and probability of f_sentence to be\n",
        "#   translated into e_sentence\n",
        "################################################################################\n",
        "\n",
        "def translation(f_sentence, e_sentence, ph_dict, n, gram_en, a):\n",
        "  # f_sentence and e_sentence are structured as phrases in list of list\n",
        "  # all possible correspondences are considered\n",
        "  corresp_dic={}\n",
        "  f_sentence = [rm_turkish(i) for i in f_sentence]\n",
        " \n",
        "  lm_p = language_model_prob(e_sentence, n, gram_en, info=True) # language model\n",
        "  if lm_p <= 0: # if probability of language model is 0, then all is 0\n",
        "    print(\"language model probability for {} is 0\".format(e_sentence))\n",
        "    return {}, 0\n",
        "  for perm in permutations(e_sentence):\n",
        "    corresp = list(zip(f_sentence, perm))\n",
        "    p=1\n",
        "    for fp, ep in corresp:\n",
        "      x = distance(f_sentence, e_sentence, fp, ep, corresp)\n",
        "      if not fp in ph_dict: # to check whether the phrase is in the phrase translation table\n",
        "        pfe=0 # if not, makes it 0\n",
        "      else:\n",
        "        if not ep in ph_dict[fp]:\n",
        "          pfe=0 \n",
        "        else:\n",
        "          pfe=ph_dict[fp][ep]\n",
        "      p*=(pfe*db_reordering(int(x), a)) # probability of correspondence without language model\n",
        "    p*=lm_p # add language model\n",
        "    corresp_dic[str(corresp)]=p # probability of correspondance\n",
        "  sump=np.sum(list(corresp_dic.values())) # find sum of all\n",
        "  for key in corresp_dic:\n",
        "    if sump!= 0:\n",
        "      corresp_dic[key]/=sump # normalize\n",
        "      \n",
        "  non_zero = {}\n",
        "  for key, val in corresp_dic.items():\n",
        "    if val != 0:\n",
        "      non_zero[key]=val\n",
        "  \n",
        "  if len(non_zero) == 0:\n",
        "    print(\"! all alignments are zero\")\n",
        "    \n",
        "  return non_zero, sump\n",
        "\n",
        "################################################################################\n",
        "\n",
        "\n",
        "############################### examples #######################################\n",
        "# The function involves examples for testing\n",
        "################################################################################\n",
        "\n",
        "def examples():\n",
        "  en_full = [[\"I am going to go to the school\"],\n",
        "             [\"I will be there, just a minute\"], \n",
        "             [\"Because, today is a good day\"],   \n",
        "            [\"This book is red, that table is blue\"]]\n",
        "  tr_full = [[\"Okula gideceğim\"], \n",
        "            [\"Birkaç dakika içinde orada olacağım\"], \n",
        "            [\"Çünkü, bugün güzel bir gün\"], \n",
        "            [\"Bu kitap kırmızı şu masa ise mavi\"]]\n",
        "  \n",
        "  tr_full = tokenization(sum(tr_full, []), \"t\")\n",
        "  \n",
        "  en_phrase = [[\"I\", \"am going to\", \"go to the\", \"school\"],\n",
        "              [\"I am\", \"going to go to\", \"the school\"],\n",
        "              [\"I\", \"will be there,\", \"just a minute\"],\n",
        "              [\"I will be there\", \"just a minute\"],\n",
        "              [\"Because,\", \"today is\", \"a good day\"],\n",
        "              [\"Because, today is\", \"a good day\"],\n",
        "              [\"This book\", \"is red,\", \"that table\", \"is blue\"],\n",
        "              [\"This\", \"book\", \"is red\", \"that\", \"table\", \"is blue\"]]\n",
        "  \n",
        "  tr_phrase = [[\"Okula\", \"gidecegim\"], [\"Okula gidecegim\"],\n",
        "              [\"Birkac dakika icinde\", \"orada olacagım\"], \n",
        "              [\"Birkac dakika\", \"icinde\", \"orada olacagım\"],\n",
        "              [\"Cunku,\", \"bugun\", \"guzel bir gun\"],\n",
        "              [\"Cunku, bugun\", \"guzel bir gun\"],\n",
        "              [\"Bu kitap\", \"kırmızı\", \"su masa ise\", \"mavi\"],\n",
        "              [\"Bu kitap kırmızı\", \"su masa ise mavi\"]]\n",
        "  \n",
        "  en_phrases_exp = [\"I am going to\", \"of course\", \"because of\", \"This book\", \"it have been otherwise\"]\n",
        "  tr_phrases_exp = [\"Cunku\", \"Elli yaslarında\", \"bilinmiyor\", \"sahtekarlık etmem.\", \"bunu size emrediyorum.\"]\n",
        "  \n",
        "  en_gram_test=[[\"<s>\", \"<s>\", \"Because\"],\n",
        "              [\"<s>\", \"A\", \"great\"],\n",
        "              [\"as\", \"it\", \"is\"],\n",
        "              [\"going\", \"to\", \"go\"],\n",
        "              [\"Who\", \"are\", \"you\"]]\n",
        "  \n",
        "  tr_gram_test=[[\"<s>\", \"<s>\", \"Ben\"],\n",
        "               [\"<s>\", \"Bu\", \"kitap\"],\n",
        "               [\"Bu\", \"bir\", \"masa\"],\n",
        "               [\"hayli\", \"zaman\", \"oldu\"],\n",
        "                [\"o\", \"kim\", \"idi\"]]\n",
        "  \n",
        "  return en_full, tr_full, en_phrase, tr_phrase, en_phrases_exp, tr_phrases_exp, en_gram_test, tr_gram_test\n",
        "\n",
        "################################################################################\n",
        "\n",
        "\n",
        "############################### return_model ###################################\n",
        "# The function returns \n",
        "#   viterbi alignments as vt2e and ve2t\n",
        "#   language model for English n-grams as lm_en\n",
        "#                  for Turkish n-grams as lm_tr\n",
        "#   and phrase translation probabilities as ph_dict\n",
        "#\n",
        "# The tables are received from respective paths in the drive\n",
        "################################################################################\n",
        "\n",
        "def return_model():\n",
        "  # data type of viterbi alignments are changed with v_to_tuple\n",
        "  # normally, they are structured as nested dictionaries\n",
        "  # however, to find intersection and union, they should be changed to be\n",
        "  # (e1, f1), (e2, f2) etc.\n",
        "  vt2e = v_to_tuple(read_viterbi_alignments(\"t2e\"))\n",
        "  ve2t = v_to_tuple(read_viterbi_alignments(\"e2t\")) \n",
        "  lm_en = read_language_model(\"english\", \"two-gram\")\n",
        "  lm_tr = read_language_model(\"turkish\", \"two-gram\")\n",
        "  ph_dict = read_phrases(\"t2e\")\n",
        "  return vt2e, ve2t, lm_en, lm_tr, ph_dict\n",
        "\n",
        "################################################################################\n",
        "\n",
        "def train(n_sample, n_gram, phrase_limit, vt2e, ve2t, key):\n",
        "  n=n_gram #n-gram\n",
        "  english_train, turkish_train, english_test, turkish_test = get_sample(n_sample) # get sample for phrase probabilities\n",
        " \n",
        "  vt2e = v_to_tuple(vt2e) # get viterbi alignments from drive\n",
        "  ve2t = v_to_tuple(ve2t) # get viterbi alignments from drive\n",
        "  \n",
        "  # finding phrase alignments and saving them as a .json document \n",
        "  # limit: the limit length of phrases to be extracted\n",
        "  ph_dict = phrase_alignment(english_train, turkish_train, vt2e, ve2t, limit=phrase_limit, op=\"t2e\"+str(key))\n",
        "  \n",
        "  # finding n-gram probabilities\n",
        "  en_lm = language_model(n, english_train, \"english\", str(n_gram)+\"-gram\"+str(key))\n",
        "  tr_lm = language_model(n, turkish_train, \"turkish\", str(n_gram)+\"-gram\"+str(key))\n",
        "  \n",
        "  return vt2e, ve2t, en_lm, tr_lm, ph_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uyt4KjfUH_jC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vt2e = read_viterbi_alignments(\"t2e\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8ou6d0hIMeW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ve2t = read_viterbi_alignments(\"e2t\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-rGikExIOpN",
        "colab_type": "code",
        "outputId": "34e9ddbd-4d86-4aa9-dee8-5cef3a64dc35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "vt2e, ve2t, en_lm, tr_lm, ph_dict = train(180000, 2, 20, vt2e, ve2t, \"\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "sentences for training have been processed\n",
            "number of phrases: 610256\n",
            "phrase probabilities are found and saved!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}