{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IBM Model2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hbasgol/ibm_models/blob/master/IBM_Model2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gn5_CHf8y9-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "###################### modules/libraries #######################################\n",
        "#\n",
        "# import os -> to see the path of folders and libraries\n",
        "#\n",
        "# import time -> to learn the timing of each step of expectation-maximization\n",
        "#\n",
        "# from google.colab import drive -> to connect google drive\n",
        "#\n",
        "# import codecs -> to save Turkish characters without problem\n",
        "#\n",
        "# import json -> to save dictionaries structured for representing tables as\n",
        "#   .json documents, json.dump is used for writing and json.loads is used for\n",
        "#   reading .json strings as dictionaries\n",
        "#\n",
        "# from itertools import product\n",
        "# from itertools import permutations -> to find possible alignments between\n",
        "#   English and Turkish phrases that structure corresponding sentences\n",
        "#\n",
        "# import numpy as np -> for simple mathematical operations such as taking absolute\n",
        "#   of a number with np.abs or summing values in a list with np.sum\n",
        "#\n",
        "# import unicodedata -> for removing Turkish characters in the corpus\n",
        "#\n",
        "################################################################################\n",
        "\n",
        "import os\n",
        "import time\n",
        "from google.colab import drive\n",
        "import codecs\n",
        "import json\n",
        "from itertools import permutations\n",
        "from itertools import product\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "\n",
        "\n",
        "################################################################################\n",
        "\n",
        "\n",
        "###################### connectcolab ############################################\n",
        "# the function connectcolab is used to receive Google Drive documents and\n",
        "#   determine the path the files are written to.\n",
        "################################################################################\n",
        "def connectcolab():\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "path = \"/content/drive/My Drive/Colab Notebooks/Machine Translation/IBM2-tables/\"\n",
        "corpus_path = \"/content/drive/My Drive/Colab Notebooks/Machine Translation/corpus/\"\n",
        "connectcolab()\n",
        "################################################################################\n",
        "\n",
        "\n",
        "###################### tokenization ############################################\n",
        "# takes sentencelist such as [[\"first sentence\"], [\"second sentence\"], [\"third sentence\"] ...] \n",
        "#   two operators named op1 and op2\n",
        "#   op1 is used to determine the translation direction whether English to Turkish\n",
        "#    or Turkish to English, because IBM 1 and IBM 2 should be run bi-directional\n",
        "#    to get word-alignments for phrase extraction in phrase-based translation\n",
        "#   op2 is used to determine the language of sentences because Turkish characters\n",
        "#    are removed due to the inconsistency in the corpus. For removing Turkish\n",
        "#    characters, a helper function named rm_turkish is used\n",
        "# The function\n",
        "#  returns [[\"first\", \"sentence\"], [\"second\", \"sentence\"], [\"third\", \"sentence\"] ...]\n",
        "#  or\n",
        "#  returns  [[\"NULL\", \"first\", \"sentence\"], [\"NULL\", \"second\", \"sentence\"] ...]\n",
        "################################################################################\n",
        "\n",
        "def tokenization(sentencelist, op1, op2):\n",
        "  if op1 == \"t2e\":\n",
        "    if op2 == \"t\":\n",
        "      return [[\"NULL\"] + [*map(rm_turkish, i.split(\" \"))] for i in sentencelist]\n",
        "    if op2 == \"e\":\n",
        "      return [i.split(\" \") for i in sentencelist]\n",
        "  if op1 == \"e2t\":\n",
        "    if op2 == \"t\":\n",
        "      return [[*map(rm_turkish, i.split(\" \"))] for i in sentencelist]\n",
        "    if op2 == \"e\":\n",
        "      return [[\"NULL\"] + i.split(\" \") for i in sentencelist]\n",
        "    \n",
        "################################################################################\n",
        "\n",
        "\n",
        "#################### rm_turkish ################################################\n",
        "# the function takes a word that is a string data type and change Turkish\n",
        "# characters into English counterparts.\n",
        "#\n",
        "# takes uçuyorum -> returns ucuyorum\n",
        "# şenlik -> senlik\n",
        "#\n",
        "# to remove the ambiguity of the corpus in terms of Turkish sentences\n",
        "#\n",
        "# the function is used in another function named tokenization\n",
        "################################################################################\n",
        "\n",
        "def rm_turkish(word):\n",
        "  normalized = unicodedata.normalize('NFD', word)\n",
        "  word = \"\".join([c for c in normalized if not unicodedata.combining(c)])\n",
        "  return word\n",
        "\n",
        "################################################################################\n",
        "\n",
        "\n",
        "\n",
        "###################### get_words ###############################################\n",
        "# get_words takes two arguments: english_list_train as target turkish_list_train as\n",
        "#    source sentence, or vice versa, which does not affect code\n",
        "#    such as [[\"first\", \"sentence\"], [\"second\", \"sentence\"], [\"third\", \"sentence\"], ...]\n",
        "#\n",
        "# get_words returns unique words as two arguments \n",
        "#    such as [[\"first\"], [\"sentence\"], [\"second\"], [\"third\"]]\n",
        "################################################################################\n",
        "\n",
        "def get_words(BU_sentences):\n",
        "  wordset=set()\n",
        "  for sentence in BU_sentences:\n",
        "    for word in sentence:\n",
        "      wordset.add(word)\n",
        "  return list(wordset)\n",
        "\n",
        "################################################################################\n",
        " \n",
        "\n",
        "########################## save/read_t_table functions #########################\n",
        "# save_t_table takes a specific dictionary, involving t_tables and writes it to\n",
        "#    the disk as a .json file with the name of \"IBM2-t_table.json\"\n",
        "# read_t_table does not take an argument, but reads \"IBM2-t_table.json\" as a\n",
        "#    dictionary for further use\n",
        "# save_alignments takes a dictionary, involving alignment probabilities writes\n",
        "#    it to the disk as a .json file with the name of \"IBM2-alignments\"\n",
        "# read_alignments does not take an argument, but reads \"IBM2-alignments.json\" \n",
        "#    as a dictionary for further use\n",
        "# save_viterbi_alignments takes a dictionary, max alignments and writes it to\n",
        "#    the disk with the name of \"IBM2-viterbi.json\" to use viterbi alignments\n",
        "#    in phrase-based translation model\n",
        "# \n",
        "# Each function has an operator named op to determine the direction of translation\n",
        "#   op can be \"t2e\" for Turkish to English and \"e2t\" for English to Turkish\n",
        "#   translation\n",
        "################################################################################\n",
        "\n",
        "def save_t_table(t_tables, op):\n",
        "  with codecs.open(path+\"IBM2-t_table\"+op+\".json\", 'w', 'utf-8') as f:\n",
        "    json.dump(obj=t_tables, fp=f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "def read_t_table(op):\n",
        "  path = \"/content/drive/My Drive/Colab Notebooks/Machine Translation/IBM1-tables/\"\n",
        "  with codecs.open(path+\"IBM1-t_table\"+op+\".json\") as f:\n",
        "    return json.loads(f.read())\n",
        "\n",
        "def save_alignments(alignments, op):\n",
        "  with codecs.open(path+\"IBM2-alignments\"+op+\".json\", 'w', 'utf-8') as f:\n",
        "    json.dump(obj=alignments, fp=f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "def read_alignments(op):\n",
        "  with codecs.open(path+\"IBM2-alignments\"+op+\".json\") as f:\n",
        "    alignments = json.loads(f.read())\n",
        "  alignments_int = {int(i): {int(j): {int(le): {int(lf): \\\n",
        "                          alignments[i][j][le][lf] \\\n",
        "                           for lf in alignments[i][j][le]} \\\n",
        "                           for le in alignments[i][j]} \\\n",
        "                           for j in alignments[i]} \\\n",
        "                           for i in alignments}\n",
        "  del alignments\n",
        "  return alignments_int\n",
        "  \n",
        "def save_viterbi_alignments(max_alignments, op):\n",
        "  with codecs.open(path+\"IBM2-viterbi\"+op+\".json\", \"w\", 'utf-8') as f:\n",
        "    json.dump(obj=max_alignments, fp=f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "################################################################################\n",
        "\n",
        "\n",
        "############################## get_t_table #####################################\n",
        "# the function takes two arguments\n",
        "#    english_list_train, turkish_list_train, turkish_tk_word)\n",
        "#    english_list_train: english sentences such as\n",
        "#       [[\"first\", \"sentence\"], [\"second\", \"sentence\"], [\"third\", \"sentence\"], ...]\n",
        "#    turkish_list_train: turkish sentences, respectively\n",
        "#\n",
        "# the function, throughout the execution, creates six tables as dictionaries\n",
        "#    count_tables, total_tables, s_totals, alignments, count_alignments, \n",
        "#    total_alignments, normally, algorithm loops on all english and turkish \n",
        "#    words, however, for memory issues, sentence matches are considered here\n",
        "#    all respective values are added respective dictionaries\n",
        "#    -alignments table is initialized uniformly with 1/lf+1, where lf is \n",
        "#        the length of foreign sentence f, turkish in this case.\n",
        "#    -count_tables, total_tables, s_totals, alignments, count_alignments, \n",
        "#        total_alignments are set to 0.\n",
        "#\n",
        "# the function returns one arguments,\n",
        "#    tablelist: the list of tables mentioned above\n",
        "#\n",
        "#    t_table is a dictionary like\n",
        "#    t_table = {\"Hello\":\n",
        "#                {Book: 0.32}\n",
        "#                {Furniture: 0.12} ...}\n",
        "#    0.32 by which is received t_table[\"Hello\"][\"Book\"] count_tables is same\n",
        "#\n",
        "#    s_totals is like\n",
        "#    s_totals = {\"Hello\": 0.218,\n",
        "#                \"Furniture\": 0.45 ...} total tables is same\n",
        "#\n",
        "#    alignments is a dictionary like\n",
        "#    (i, j, le, lf) --> set((1, 2, 5, 7), (1, 2, 8, 5).....)\n",
        "#    alignments = {i: {j: {le: {lf: value} ...} ...} ...}\n",
        "#               --> {1: {2: {5: {7: value} ...} ...} ...}\n",
        "#    which values can be received by alignments[1][2][5][7]\n",
        "# other alignment tables are structured like the alignments table\n",
        "# such as total alignments, one can receive \n",
        "# total alignments = {j: {le: {lf: value} ...} ...}\n",
        "#                totalalignments[j][le][lf]\n",
        "################################################################################\n",
        "\n",
        "def get_t_table(english_list_train, turkish_list_train):\n",
        "  #empty dictionaries are genereated\n",
        "  count_tables, total_tables, s_totals, alignments, count_alignments, total_alignments = {}, {}, {}, {}, {}, {}\n",
        "  for index_e, e_sentence in enumerate(english_list_train): # for each english sentence\n",
        "    f_sentence = turkish_list_train[index_e] # determine foreign sentence with respective index\n",
        "    le = len(e_sentence)\n",
        "    lf = len(f_sentence)\n",
        "    for j, e_word in enumerate(e_sentence):\n",
        "      if j not in total_alignments:\n",
        "        total_alignments[j]={}\n",
        "      if le not in total_alignments[j]:\n",
        "          total_alignments[j][le]={}\n",
        "      total_alignments[j][le][lf]=0 # creating nested dictionaries\n",
        "      if e_word not in count_tables:\n",
        "        count_tables[e_word]={}\n",
        "      s_totals[e_word]=0\n",
        "      for i, f_word in enumerate(f_sentence):\n",
        "        if i not in alignments:\n",
        "          alignments[i]={}\n",
        "          count_alignments[i]={}\n",
        "        if j not in alignments[i]:\n",
        "          alignments[i][j]={}\n",
        "          count_alignments[i][j]={}\n",
        "        if le not in alignments[i][j]:\n",
        "          alignments[i][j][le]={}\n",
        "          count_alignments[i][j][le]={}\n",
        "        alignments[i][j][le][lf]=1/lf+1\n",
        "        count_alignments[i][j][le][lf]=0 # creating nested dictionaries\n",
        "        total_tables[f_word]=0\n",
        "        count_tables[e_word].update({f_word: 0}) # creating nested dictionaries\n",
        "        \n",
        "  tablelist = [count_tables, total_tables, s_totals, alignments, count_alignments, total_alignments]\n",
        "  print(\"tables have been created and saved\")\n",
        "  return tablelist\n",
        "\n",
        "################################################################################\n",
        "\n",
        "\n",
        "############################## to_zero #########################################\n",
        "# the function takes four arguments, which are tables structured as nested\n",
        "#   dictionaries. The tables are \n",
        "#   count_tables, total_tables, count_alignments, total_alignments\n",
        "#\n",
        "# it changes the values of count_tables, total_tables, count_alignments, \n",
        "#   total_alignments to be 0, which is a step of expectation_maximization.\n",
        "#\n",
        "# Then, it returns these tables as a list:\n",
        "#   [count_tables, total_tables, s_totals, count_alignments, total_alignments] \n",
        "################################################################################\n",
        "\n",
        "def to_zero(count_tables, total_tables, count_alignments, total_alignments):\n",
        "  for e_word, val in count_tables.items():\n",
        "    for f_word, value in val.items():\n",
        "      total_tables[f_word]=0\n",
        "      count_tables[e_word][f_word]=0\n",
        "  for i, iv in count_alignments.items():\n",
        "    for j, jv in iv.items():\n",
        "      for le, lev in jv.items():\n",
        "        for lf, lfv in lev.items():\n",
        "          count_alignments[i][j][le][lf]=0\n",
        "          total_alignments[j][le][lf]=0\n",
        "  return [count_tables, total_tables, count_alignments, total_alignments]\n",
        "\n",
        "################################################################################\n",
        "\n",
        "\n",
        "############################### get_sample #####################################\n",
        "# The function takes one argument, n_sample, which determines the number of \n",
        "#   sentences which will be used. The n_sample of sentences\n",
        "#   are then separated into test and train sets with a ratio of 1/9\n",
        "#\n",
        "# The function returns english_train, turkish_train, english_test, turkish_test, \n",
        "#   english_word, turkish_word\n",
        "#   english_word, turkish_word: unique words in the corpus\n",
        "#   english_train: english sentences the model to be trained as a list of list\n",
        "#   turkish_train: turkish sentences the model to be trained as a list of list\n",
        "#   english_test: english sentences the model to be tested as a list of list\n",
        "#   english_test: english sentences the model to be tested as a list of list\n",
        "#\n",
        "# The function named connectcolab() used in this function to connect Google Drive\n",
        "# The function named tokenization() tokenizes sentences\n",
        "# The function named get_words() is used to get unique words\n",
        "################################################################################\n",
        "\n",
        "def get_sample(sample, op1):\n",
        "  connectcolab()\n",
        "    \n",
        "  english_list = []\n",
        "  with open(corpus_path+\"english.txt\", \"r\") as english:\n",
        "    for cnt, line in enumerate(english):\n",
        "      english_list.append(line.rstrip())\n",
        "\n",
        "  turkish_list = []\n",
        "  with open(corpus_path+\"turkish.txt\", \"r\") as turkish:\n",
        "    for cnt, line in enumerate(turkish):\n",
        "      turkish_list.append(line.rstrip())\n",
        "  \n",
        "  english_list=english_list[:sample]\n",
        "  turkish_list=turkish_list[:sample]\n",
        "  \n",
        "  rtrain = int(sample*0.9)\n",
        "\n",
        "  english_list = tokenization(english_list, op1, \"e\")\n",
        "  turkish_list = tokenization(turkish_list, op1, \"t\")\n",
        "  english_word = get_words(english_list)\n",
        "  turkish_word = get_words(turkish_list)\n",
        "  english_train, turkish_train = english_list[:rtrain], turkish_list[:rtrain]\n",
        "  english_test, turkish_test = english_list[rtrain:], turkish_list[rtrain:]\n",
        "  del english_list\n",
        "  del turkish_list\n",
        "  print(\"sentences for training have been processed\")\n",
        "  return english_train, turkish_train, english_test, turkish_test, english_word, turkish_word\n",
        "\n",
        "################################################################################\n",
        "\n",
        "\n",
        "############################ viterbi_alignment #################################\n",
        "# a part of the application project is implementing a phrase-based model\n",
        "#    to find the phrases, corpus should be word-aligned; to do this,\n",
        "#    viterbi algorithm should be run in expectation maximization algorithm\n",
        "#\n",
        "# viterbi_alignment takes english_train, turkish_train, t_tables, alignments, op\n",
        "#    english_train: list of English sentences structured as list of list\n",
        "#    turkish_train: list of Turkish sentences structured as list of list\n",
        "#    t_tables: a dictionary involving translation probabilities\n",
        "#    alignments: a dictionary involving alignment probabilities\n",
        "#    op: an operator for determining the translation direction and saving\n",
        "#      it can be \"t2e\" or \"e2t\"\n",
        "#\n",
        "# the function does not return a variable but saves viterbi alignments as a\n",
        "#    .json file, with the help of save_viterbi_alignments() function\n",
        "################################################################################\n",
        "\n",
        "def viterbi_alignment(english_train, turkish_train, t_tables, alignments, op):\n",
        "  total_a = {}\n",
        "  for e_index, e_sentence in enumerate(english_train):\n",
        "    f_sentence = turkish_train[e_index]\n",
        "    le = len(e_sentence)\n",
        "    lf = len(f_sentence)\n",
        "    max_a = {}\n",
        "    for j, e_word in enumerate(e_sentence):\n",
        "      current_max = (0, -1)\n",
        "      for i, f_word in enumerate(f_sentence):\n",
        "        t_table = t_tables[e_word][f_word]\n",
        "        alignment = alignments[i][j][le][lf]\n",
        "        val = t_table*alignment\n",
        "        if current_max[1] < val:\n",
        "          current_max = (i, val)\n",
        "        max_a.update({j: current_max[0]})\n",
        "    total_a[e_index]=max_a\n",
        "  save_viterbi_alignments(total_a, op)\n",
        "  return total_a\n",
        "  \n",
        "################################################################################\n",
        "\n",
        "\n",
        "############################ perplexity ########################################\n",
        "# The function takes four arguments:\n",
        "#   english_train: English sentences\n",
        "#   turkish_train: Turkish sentences\n",
        "#   t_tables: translation probability table, word-based\n",
        "#   alignments: alignments\n",
        "#\n",
        "# It returns perplexity, which is a float number representing how well the\n",
        "#   probabilities that have been found are suited for the data\n",
        "#   In each iteration of the expectation maximization algorithm, perplexity\n",
        "#   reduces. If it does not change through iterations, this means that \n",
        "#   the algorithm converged.\n",
        "#\n",
        "# This function does not compute the whole perplexity formula, because it is\n",
        "#   computationally inefficient, rather than computing the translation probability\n",
        "#   of a sentence, it looks argmax for each word. Although the computation\n",
        "#   is not same, it gives an idea regarding the convergence of the algorithm\n",
        "################################################################################\n",
        "\n",
        "def perplexity(english_train, turkish_train, t_tables, alignments):\n",
        "  s=0\n",
        "  for index_e, e_sentence in enumerate(english_train):\n",
        "    f_sentence = turkish_train[index_e]\n",
        "    le=len(e_sentence)\n",
        "    lf=len(f_sentence)\n",
        "    try:\n",
        "      s+=np.log(max([t_tables[e_word][f_word] \\\n",
        "                 *alignments[i][j][le][lf] \\\n",
        "                 for j, e_word in enumerate(e_sentence) \\\n",
        "                 for i, f_word in enumerate(f_sentence) \\\n",
        "                 if le in alignments[i][j]              \\\n",
        "                 if lf in alignments[i][j][le]]))\n",
        "    except:\n",
        "      pass\n",
        "  return -s\n",
        "\n",
        "################################################################################\n",
        "\n",
        "\n",
        "###################### expectation_maximization ################################\n",
        "# The function takes english_word, turkish_word, english_train, turkish_train, tablelist, \n",
        "#   t_tables and op\n",
        "#   english_word: a list of english words in the corpus\n",
        "#   turkish_word: a list of turkish words in the corpus\n",
        "#   english_train: english sentences the model to be trained as a list of list\n",
        "#   turkish_train: turkish sentences the model to be trained as a list of list\n",
        "#   tablelist: is a list of dictionaries created by get_t_table\n",
        "#      count_tables, total_tables, s_totals, alignments, count_alignments, \n",
        "#      total_alignments\n",
        "#   t_tables: is a dictionary, which involves translation probabilities\n",
        "#   alignment_set: is a dictionary involving all alignments in the data set\n",
        "#\n",
        "# The function returns translation probabilities named t_table, \n",
        "#   alignment probabilities named alignments, which will be used to test the\n",
        "#   model\n",
        "#\n",
        "# Throughout the execution, the function looks at perplexity and saves\n",
        "#   t_tables, alignments and max_alignments (viterbi alignments) as .json\n",
        "#   file format.\n",
        "################################################################################\n",
        "\n",
        "def expectation_maximization(english_word, turkish_word, english_train, turkish_train, tablelist, t_tables, op):\n",
        "  [count_tables, total_tables, s_totals, alignments, count_alignments, total_alignments] = tablelist\n",
        "  count = 1                                      # to count how many steps taken\n",
        "  epsilon = 1                                    # to determine convergence, tolerance\n",
        "  lastval = 10\n",
        "  count_p=0\n",
        "  s=0\n",
        "  print(\"number of en sentences {}, tr sentences {}, en words {}, tr words {}\".format(len(english_train), len(turkish_train), len(turkish_word), len(english_word)))\n",
        "  while True:\n",
        "    print(\"✖ step {}\".format(count))\n",
        "    start = time.time()                          # for timing each step, start\n",
        "    if count > 1:\n",
        "      [count_tables, total_tables, count_alignments, total_alignments] \\\n",
        "      = to_zero(count_tables, total_tables, count_alignments, total_alignments)\n",
        "    for index_e, e_sentence in enumerate(english_train):\n",
        "      f_sentence = turkish_train[index_e]\n",
        "      le = len(e_sentence)\n",
        "      lf = len(f_sentence)\n",
        "      for j, e_word in enumerate(e_sentence):\n",
        "        s_totals[e_word]=0\n",
        "        for i, f_word in enumerate(f_sentence):\n",
        "          s_totals[e_word]+=t_tables[e_word][f_word]*alignments[i][j][le][lf]\n",
        "      for j, e_word in enumerate(e_sentence):  \n",
        "        for i, f_word in enumerate(f_sentence):\n",
        "          c = t_tables[e_word][f_word]*alignments[i][j][le][lf]/s_totals[e_word]\n",
        "          count_tables[e_word][f_word]+=c\n",
        "          count_alignments[i][j][le][lf]+=c\n",
        "          total_tables[f_word]+=c\n",
        "          total_alignments[j][le][lf]+=c\n",
        "    t_tables.update({e_word: {f_word:\\\n",
        "                         count_tables[e_word][f_word]/total_tables[f_word]\n",
        "                         for f_word in t_tables[e_word]}\\\n",
        "                         for e_word in t_tables})\n",
        "    alignments.update({i: {j: {le: {lf: \\\n",
        "                              count_alignments[i][j][le][lf] \\\n",
        "                              /total_alignments[j][le][lf] \\\n",
        "                               for lf in alignments[i][j][le]} \\\n",
        "                               for le in alignments[i][j]} \\\n",
        "                               for j in alignments[i]} \\\n",
        "                               for i in alignments})\n",
        "    \n",
        "    rv_table, t_tables = normalize(t_tables)\n",
        "    del rv_table\n",
        "    rv_alignments, alignments = normalize_alignments(alignments)\n",
        "    del rv_alignments\n",
        "    s = perplexity(english_train, turkish_train, t_tables, alignments)\n",
        "    print(\"perplexity: {}, difference in perplexity: {}\".format(s, s-lastval))\n",
        "    \n",
        "    if count % 3 == 0:\n",
        "      max_alignment = viterbi_alignment(english_train, turkish_train, t_tables, alignments, op)\n",
        "      save_t_table(t_tables, op)\n",
        "      save_alignments(alignments, op)\n",
        "      \n",
        "    if s-lastval >= 0:\n",
        "      max_alignment = viterbi_alignment(english_train, turkish_train, t_tables, alignments, op)\n",
        "      save_t_table(t_tables, op)\n",
        "      save_alignments(alignments, op)\n",
        "      break\n",
        "  \n",
        "    lastval=s\n",
        "    count+=1\n",
        "    end = time.time()\n",
        "    print(\"execution time for one step: {}\".format(end-start))\n",
        "  return t_tables, alignments, max_alignment, rv_table, rv_alignments\n",
        "\n",
        "################################################################################\n",
        "\n",
        "\n",
        "################################ prob_al #######################################\n",
        "# The function takes five arguments:\n",
        "#   t_tables, alignments, t_sentence as a Turkish sentence\n",
        "#   e_sentence as an English sentence, op an operator to determine the direction\n",
        "#   of translation.\n",
        "#\n",
        "# The function finds all probable alignments between source and target sentences\n",
        "#   and returns a normalized probability for each of them with a nested dictionary\n",
        "# Additionally, it sums up probabilities of all probable alignments to find\n",
        "#   the translation probability of source and target sentence\n",
        "################################################################################\n",
        "\n",
        "def prob_al(t_tables, alignments, t_sentence, e_sentence, op):\n",
        "  t_sentence = tokenization([t_sentence], op, \"t\")[0]\n",
        "  e_sentence = tokenization([e_sentence], op, \"e\")[0]\n",
        "  if op==\"t2e\":         # table is t[e_word][f_word] or t[target][source]\n",
        "    source = t_sentence # determine source and target sentences\n",
        "    target = e_sentence\n",
        "  else:\n",
        "    source = e_sentence\n",
        "    target = t_sentence\n",
        "\n",
        "  print(\"source sentence:\", source)\n",
        "  print(\"target sentence:\", target)\n",
        "\n",
        "  al=[]\n",
        "  for t in [\" \"]+target: # source can be not-aligned\n",
        "    wal=[]\n",
        "    for s in source:\n",
        "      wal.append([s, t]) # probable alignments\n",
        "    al.append(wal)\n",
        "    \n",
        "  ali={}\n",
        "  ind=0\n",
        "\n",
        "  for a in product(*al): # find sentence alignments\n",
        "                                   # target should have one alignment\n",
        "    if len(set([l[1] for l in a])) == len(target)+1:\n",
        "      p_a=1\n",
        "      for l in a:\n",
        "        if l[1]!=\" \":\n",
        "          # translation and alignment prob.\n",
        "          if l[1] in t_tables and l[0] in t_tables[l[1]]:\n",
        "            try:\n",
        "              alp = alignments[source.index(l[0])][target.index(l[1])][len(source)][len(target)]\n",
        "              if alp == 0:\n",
        "                alp=1e-50 # if the probability of alignment is 0, just show it\n",
        "            except:\n",
        "              alp=1e-10 # if it cannot find the alignment from the corpus\n",
        "            p_a*=(t_tables[l[1]][l[0]]*alp)\n",
        "          else:\n",
        "            p_a*=0\n",
        "      if p_a!=0:\n",
        "        ali[ind]=[a, p_a]\n",
        "    ind+=1\n",
        "\n",
        "  p_sum=0\n",
        "  for key in ali:\n",
        "    p_sum+=ali[key][1] # sum over probabilities\n",
        "    \n",
        "  for key in ali:\n",
        "    if p_sum!=0:\n",
        "      ali[key][1]/=p_sum # normalization\n",
        "  \n",
        "  values = list(set([ali[key][1] for key in ali]))\n",
        "  values.sort(reverse = True)\n",
        "  \n",
        "  sortedali = {}\n",
        "  for val in values[:2]:\n",
        "      for key in ali:\n",
        "          if ali[key][1] == val:\n",
        "              sortedali[key]=ali[key]\n",
        "   \n",
        "  return sortedali, p_sum\n",
        "  \n",
        "################################################################################\n",
        "\n",
        "\n",
        "################################ prob_word #####################################\n",
        "# The function takes three arguments\n",
        "#   t_table as a t_table\n",
        "#   word as a word in the corpus\n",
        "#   number is the maximum n probable words for word\n",
        "#\n",
        "# The function returns a list of tuples\n",
        "#   such that \n",
        "#   maxtuplist=[(ev, house, 0.5), \n",
        "#               (ev, home, 0.4), \n",
        "#               (ev, book, 0.001) ...]\n",
        "################################################################################\n",
        "\n",
        "def prob_word(t_table, word, n):\n",
        "  tuplist=[]\n",
        "  maxtuplist=[]\n",
        "  for t_word, t_values in t_table.items():\n",
        "    for s_word, s_values in t_values.items():\n",
        "      if word == s_word:\n",
        "        tuplist.append((s_word, t_word, s_values))\n",
        "  values=sorted([i[2] for i in tuplist])[:n-1]\n",
        "  for val in values:\n",
        "    for tup in tuplist:\n",
        "      if tup[2]==val:\n",
        "        maxtuplist.append(tup)\n",
        "  return maxtuplist\n",
        "################################################################################\n",
        "\n",
        "\n",
        "############################### examples #######################################\n",
        "# The function involves examples for testing\n",
        "################################################################################\n",
        "\n",
        "def examples():\n",
        "  en_full = [[\"I am going to go to the school\"],\n",
        "             [\"I will be there, just a minute\"], \n",
        "             [\"Because, today is a good day\"],   \n",
        "            [\"This book is red, that table is blue\"]]\n",
        "  tr_full = [[\"Okula gidecegim\"], \n",
        "            [\"Birkac dakika icinde orada olacagım\"], \n",
        "            [\"Cunku, bugun guzel bir gun\"], \n",
        "            [\"Bu kitap kırmızı su masa ise mavi\"]]\n",
        "\n",
        "  en_word = [[\"table\"], [\"black\"], [\"computer\"], [\"So\"], \n",
        "             [\"slim\"], [\"paper\"], [\"team\"], [\"accomplishment\"]]\n",
        "  \n",
        "  tr_word = [[\"masa\"], [\"siyah\"], [\"bilgisayar\"], [\"Yani\"], \n",
        "             [\"ince\"], [\"kagıt\"], [\"takım\"], [\"basarı\"]]\n",
        "  \n",
        "  return en_full, tr_full, en_word, tr_word\n",
        "\n",
        "################################################################################\n",
        "\n",
        "\n",
        "############################### return_model ###################################\n",
        "# The function returns \n",
        "#   t_tables and alignments\n",
        "#\n",
        "# The tables are received from respective paths in the drive\n",
        "################################################################################\n",
        "\n",
        "def return_model(op):\n",
        "  t_tables = read_t_table(op)\n",
        "  alignments = read_alignments(op)\n",
        "  return t_tables, alignments\n",
        "\n",
        "################################################################################\n",
        "\n",
        "def table_rv(t_tables):\n",
        "  rv_table={}\n",
        "  for key, val in t_tables.items():\n",
        "    for k2, v2 in val.items():\n",
        "      if k2 not in rv_table:\n",
        "        rv_table[k2]={}\n",
        "      rv_table[k2][key]=v2\n",
        "  return rv_table\n",
        "\n",
        "def normalize(t_table):\n",
        "  rv_table = table_rv(t_table)\n",
        "  for f, e in rv_table.items():\n",
        "    n = sum(list(e.values()))\n",
        "    for ew in e:\n",
        "      e[ew]/=n\n",
        "  for e, f in t_table.items():\n",
        "    for f_word in f:\n",
        "      t_table[e][f_word]=rv_table[f_word][e]\n",
        "  return rv_table, t_table\n",
        "\n",
        "def normalize_alignments(alignments):\n",
        "  rv_alignments={}\n",
        "  for i in alignments:\n",
        "    for j in alignments[i]:\n",
        "      if j not in rv_alignments:\n",
        "        rv_alignments[j]={}\n",
        "      for le in alignments[i][j]:\n",
        "        if le not in rv_alignments[j]:\n",
        "          rv_alignments[j][le]={}\n",
        "        for lf, val in alignments[i][j][le].items():\n",
        "          if lf not in rv_alignments[j][le]:\n",
        "            rv_alignments[j][le][lf]={}\n",
        "          rv_alignments[j][le][lf][i]=val\n",
        "  for j in rv_alignments:\n",
        "    for le in rv_alignments[j]:\n",
        "      for lf in rv_alignments[j][le]:\n",
        "        n = sum(list(rv_alignments[j][le][lf].values()))\n",
        "        for i in rv_alignments[j][le][lf]:\n",
        "          if n != 0:\n",
        "            rv_alignments[j][le][lf][i]/=n\n",
        "  for i in alignments:\n",
        "    for j in alignments[i]:\n",
        "      for le in alignments[i][j]:\n",
        "        for lf in alignments[i][j][le]:\n",
        "          alignments[i][j][le][lf]=rv_alignments[j][le][lf][i]\n",
        "  return rv_alignments, alignments\n",
        "\n",
        "\n",
        "def continue_training(op):\n",
        "  n_sample=180000 # the sample was 180000, run until convergence\n",
        "  english_train, turkish_train, english_test, \\\n",
        "                 turkish_test, english_word, turkish_word = get_sample(n_sample, op)\n",
        "    \n",
        "  if op == \"e2t\":\n",
        "    tablelist = get_t_table(turkish_train, english_train) #form tables\n",
        "    tablelist[3] = read_alignments(op) #read alignments having been saved\n",
        "    t_tables = read_t_table(op) # read t table having been saved\n",
        "\n",
        "    t_tables, alignments, max_alignment, rv_table, rv_alignments = expectation_maximization(turkish_word, english_word, \\\n",
        "                                                    turkish_train, english_train, \\\n",
        "                                                    tablelist, t_tables, op)\n",
        "  elif op==\"t2e\":\n",
        "    tablelist = get_t_table(english_train, turkish_train) #form tables\n",
        "    tablelist[3] = read_alignments(op) #read alignments having been saved\n",
        "    t_tables = read_t_table(op) # read t table having been saved\n",
        "\n",
        "    t_tables, alignments, max_alignment, rv_table, rv_alignments = expectation_maximization(english_word, turkish_word, \\\n",
        "                                                    english_train, turkish_train, \\\n",
        "                                                    tablelist, t_tables, op)\n",
        "    \n",
        "  return t_tables, alignments, max_alignment, rv_table, rv_alignments\n",
        "  \n",
        "def train(n_sample, d, t, t_tables):\n",
        "  english_train, turkish_train, english_test, turkish_test, english_word, turkish_word = get_sample(n_sample, t)\n",
        "  print(\"Number of English Sentence {}, Turkish Sentence {}\".format(len(english_train), len(turkish_train)))\n",
        "  print(\"Number of English Word {}, Turkish Word {}\".format(len(english_word), len(turkish_word)))\n",
        "  if t == \"t2e\":\n",
        "    tablelist = get_t_table(english_train, turkish_train)\n",
        "    t_tables, alignments, max_alignment, rv_table, rv_alignments = expectation_maximization(english_word, turkish_word, \\\n",
        "                                                  english_train, turkish_train, \\\n",
        "                                                  tablelist, t_tables, d)\n",
        "  elif t == \"e2t\":\n",
        "    tablelist = get_t_table(english_train, turkish_train)\n",
        "    t_tables, alignments, max_alignment, rv_table, rv_alignments = expectation_maximization(english_word, turkish_word, \\\n",
        "                                                  english_train, turkish_train, \\\n",
        "                                                  tablelist, t_tables, d)\n",
        "  return t_tables, alignments, max_alignment, rv_table, rv_alignments"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Kl1BoLUWVL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "op = \"e2t\"\n",
        "n_sample=180000\n",
        "english_train, turkish_train, english_test, \\\n",
        "                 turkish_test, english_word, turkish_word = get_sample(n_sample, op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krYaSiWHVjHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tablelist = get_t_table(turkish_train, english_train) #form tables\n",
        "tablelist[3] = read_alignments(op) #read alignments having been saved\n",
        "t_tables = read_t_table(op) # read t table having been saved\n",
        "\n",
        "t_tables, alignments, max_alignment, rv_table, rv_alignments = expectation_maximization(turkish_word, english_word, \\\n",
        "                                                turkish_train, english_train, \\\n",
        "                                                tablelist, t_tables, op+\"-with-old\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}