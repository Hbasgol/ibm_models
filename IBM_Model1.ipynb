{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IBM Model1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hbasgol/ibm_models/blob/master/IBM_Model1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTC7tcXbp-pC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "###################### modules/libraries #######################################\n",
        "#\n",
        "# import os -> to see the path of folders and libraries\n",
        "#\n",
        "# import time -> to learn the timing of each step of expectation-maximization\n",
        "#\n",
        "# from google.colab import drive -> to connect google drive\n",
        "#\n",
        "# import codecs -> to save Turkish characters without a problem\n",
        "#\n",
        "# import json -> to save dictionaries structured for representing tables as\n",
        "#   .json documents, json.dump is used for writing and json.loads is used for\n",
        "#   reading .json strings as dictionaries\n",
        "#\n",
        "# from itertools import product\n",
        "# from itertools import permutations -> to find possible alignments between\n",
        "#   English and Turkish phrases that structure corresponding sentences\n",
        "#\n",
        "# import numpy as np -> for simple mathematical operations such as taking absolute\n",
        "#   of a number with np.abs or summing values in a list with np.sum\n",
        "#\n",
        "# import unicodedata -> for removing Turkish characters in the corpus\n",
        "#\n",
        "################################################################################\n",
        "import os\n",
        "import time\n",
        "from google.colab import drive\n",
        "import codecs\n",
        "import json\n",
        "from itertools import permutations\n",
        "from itertools import product\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "################################################################################\n",
        "\n",
        "\n",
        "###################### connectcolab ############################################\n",
        "# the function connectcolab is used to receive Google Drive documents and\n",
        "#   determine the path the files are written to and read from.\n",
        "################################################################################\n",
        "def connectcolab():\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  os.chdir(\"/content/drive/My Drive/Colab Notebooks/Machine Translation/\")\n",
        "path = \"/content/drive/My Drive/Colab Notebooks/Machine Translation/IBM1-tables/\"\n",
        "################################################################################\n",
        "\n",
        "\n",
        "###################### tokenization ############################################\n",
        "# takes sentencelist such as [[\"first sentence\"], [\"second sentence\"], [\"third sentence\"] ...] \n",
        "#   two operators named op1 and op2\n",
        "#   op1 is used to determine the translation direction whether English to Turkish\n",
        "#    or Turkish to English, because IBM 1 and IBM 2 should be run bi-directional\n",
        "#    to get word-alignments for phrase extraction in phrase-based translation\n",
        "#   op2 is used to determine the language of sentences because Turkish characters\n",
        "#    are removed due to the inconsistency in the corpus. For removing Turkish\n",
        "#    characters, a helper function named rm_turkish is used\n",
        "# The function\n",
        "#  returns [[\"first\", \"sentence\"], [\"second\", \"sentence\"], [\"third\", \"sentence\"] ...]\n",
        "#  or\n",
        "#  returns  [[\"NULL\", \"first\", \"sentence\"], [\"NULL\", \"second\", \"sentence\"] ...]\n",
        "################################################################################\n",
        "\n",
        "def tokenization(sentencelist, op1, op2):\n",
        "  if op1 == \"t2e\":\n",
        "    if op2 == \"t\":\n",
        "      return [[\"NULL\"] + [*map(rm_turkish, i.split(\" \"))] for i in sentencelist]\n",
        "    if op2 == \"e\":\n",
        "      return [i.split(\" \") for i in sentencelist]\n",
        "  if op1 == \"e2t\":\n",
        "    if op2 == \"t\":\n",
        "      return [[*map(rm_turkish, i.split(\" \"))] for i in sentencelist]\n",
        "    if op2 == \"e\":\n",
        "      return [[\"NULL\"] + i.split(\" \") for i in sentencelist]\n",
        "    \n",
        "################################################################################\n",
        "\n",
        "\n",
        "#################### rm_turkish ################################################\n",
        "# the function takes a word that is a string data type and change Turkish\n",
        "# characters into English counterparts.\n",
        "#\n",
        "# takes uçuyorum -> returns ucuyorum\n",
        "# şenlik -> senlik\n",
        "#\n",
        "# to remove the ambiguity of the corpus in terms of Turkish sentences\n",
        "#\n",
        "# the function is used in another function named tokenization\n",
        "################################################################################\n",
        "\n",
        "def rm_turkish(word):\n",
        "  normalized = unicodedata.normalize('NFD', word)\n",
        "  word = \"\".join([c for c in normalized if not unicodedata.combining(c)])\n",
        "  return word\n",
        "\n",
        "################################################################################\n",
        "\n",
        "\n",
        "###################### get_words ###############################################\n",
        "# get_words takes two arguments: english_list_train as target turkish_list_train as\n",
        "#    source sentence, or vice versa, which does not affect code\n",
        "#    such as [[\"first\", \"sentence\"], [\"second\", \"sentence\"], [\"third\", \"sentence\"], ...]\n",
        "#\n",
        "# get_words returns unique words as two arguments \n",
        "#    such as [[\"first\"], [\"sentence\"], [\"second\"], [\"third\"]]\n",
        "################################################################################\n",
        "\n",
        "def get_words(BU_sentences):\n",
        "  wordset=set()\n",
        "  for sentence in BU_sentences:\n",
        "    for word in sentence:\n",
        "      wordset.add(word)\n",
        "  return list(wordset)\n",
        "\n",
        "################################################################################\n",
        "\n",
        "\n",
        "############################## get_t_table #####################################\n",
        "# the function takes three arguments\n",
        "#    english_list_train, turkish_list_train, turkish_tk_word)\n",
        "#    english_list_train: english sentences such as\n",
        "#       [[\"first\", \"sentence\"], [\"second\", \"sentence\"], [\"third\", \"sentence\"], ...]\n",
        "#    turkish_list_train: turkish sentences, respectively\n",
        "#    turkish_tk_word: turkish words such as [[\"birinci\"], [\"cümle\"], [\"ikinci\"], [\"üçüncü\"]]\n",
        "#       for determining the values of translation table uniformly\n",
        "#\n",
        "# the function, throughout the execution, creates four tables as dictionaries\n",
        "#    t_table, count_tables, s_totals, total_tables\n",
        "#    normally, algorithm loops on all english and turkish words, however,\n",
        "#    for memory issues, sentence matches are considered here\n",
        "#    all respective values are added respective dictionaries\n",
        "#    -t_table is initialized uniformly with val = 1/len(turkish_tk_word)\n",
        "#    -count_tables, s_totals, total_tables are set to 0\n",
        "#\n",
        "# the function returns one arguments,\n",
        "#    tablelist: which is a list involving four dictionaries:\n",
        "#    t_table, count_tables, s_totals, total_tables\n",
        "#\n",
        "#    t_table is a dictionary like\n",
        "#    t_table = {\"Hello\":\n",
        "#                {Book: 0.32}\n",
        "#                {Furniture: 0.12} ...}\n",
        "#    0.32 by which is received t_table[\"Hello\"][\"Book\"] count_tables is same\n",
        "#    s_totals is like\n",
        "#    s_totals = {\"Hello\": 0.218,\n",
        "#                \"Furniture\": 0.45 ...} total tables is same\n",
        "################################################################################\n",
        "\n",
        "def get_t_table(english_list_train, turkish_list_train, turkish_tk_word):\n",
        "  #empty dictionaries are genereated\n",
        "  t_table, count_tables, s_totals, total_tables = {}, {}, {}, {}\n",
        "  val = 1/len(turkish_tk_word)\n",
        "  for index_e, e_sentence in enumerate(english_list_train): # for each english sentence\n",
        "    f_sentence = turkish_list_train[index_e] # determine foreign sentence with respective index\n",
        "    for j, e_word in enumerate(e_sentence):\n",
        "      if e_word not in count_tables:\n",
        "        count_tables[e_word]={}\n",
        "        t_table[e_word]={}\n",
        "      s_totals[e_word]=0\n",
        "      for i, f_word in enumerate(f_sentence):\n",
        "        total_tables[f_word]=0\n",
        "        count_tables[e_word].update({f_word: 0})\n",
        "        t_table[e_word].update({f_word: val})\n",
        "  tablelist = [t_table, count_tables, s_totals, total_tables] # create list\n",
        "  return tablelist\n",
        "\n",
        "################################################################################\n",
        "\n",
        "\n",
        "############################## save/read_t_table ###############################\n",
        "# save_t_table takes a specific dictionary, involving t_tables and writes it to\n",
        "#    the disk as a .json file with the name of \"IBM1-t_table.json\"\n",
        "# read_t_table does not take an argument, but reads \"IBM1-t_table.json\" as a\n",
        "#    dictionary for further use\n",
        "#\n",
        "# these functions have an operator argument named op that takes the\n",
        "#    direction of translation, which can be either \"t2e\" or \"e2t\"\n",
        "################################################################################\n",
        "\n",
        "def save_t_table(t_tables, op):\n",
        "  with codecs.open(path+\"IBM1-t_table\"+op+\".json\", 'w', 'utf-8') as f:\n",
        "    f.write(json.dumps(t_tables, indent=2, ensure_ascii=False))\n",
        "    \n",
        "def read_t_table(op):\n",
        "  with codecs.open(path+\"IBM1-t_table\"+op+\".json\") as f:\n",
        "    return json.loads(f.read())\n",
        "  \n",
        "################################################################################\n",
        "\n",
        "\n",
        "############################## to_zero #########################################\n",
        "# the function takes two arguments structured as nested dictionaries\n",
        "#   count_tables, total_tables\n",
        "#\n",
        "# it changes the values of count_tables, total_tables to be 0, \n",
        "# \twhich is a step of expectation_maximization.\n",
        "#\n",
        "# Then, it returns these tables as a list:\n",
        "#   [count_tables, total_tables] \n",
        "################################################################################\n",
        "def to_zero(count_tables, total_tables):\n",
        "  for e_word, val in count_tables.items():\n",
        "    for f_word, value in val.items():\n",
        "      total_tables[f_word]=0\n",
        "      count_tables[e_word][f_word]=0\n",
        "  return [count_tables, total_tables]\n",
        "################################################################################\n",
        "\n",
        "\n",
        "############################### get_sample #####################################\n",
        "# the functiont takes one argument, n_sample, which determines the number of \n",
        "#   sentences which will be using in the function. The n_sample of sentences\n",
        "#   are then separated into test and train sets with a ratio of 1/9\n",
        "# the function returns english_train, turkish_train, english_test, turkish_test, \n",
        "#   english_word, turkish_word\n",
        "#   english_train: english sentences the model to be trained as a list of list\n",
        "#   turkish_train: turkish sentences the model to be trained as a list of list\n",
        "#   english_test: english sentences the model to be tested as a list of list\n",
        "#   english_test: english sentences the model to be tested as a list of list\n",
        "################################################################################\n",
        "def get_sample(sample, t):\n",
        "  connectcolab()\n",
        "    \n",
        "  english_list = []\n",
        "  with open(\"/content/drive/My Drive/Colab Notebooks/Machine Translation/corpus/english.txt\", \"r\") as english:\n",
        "    for cnt, line in enumerate(english):\n",
        "      english_list.append(line.rstrip())\n",
        "\n",
        "  turkish_list = []\n",
        "  with open(\"/content/drive/My Drive/Colab Notebooks/Machine Translation/corpus/turkish.txt\", \"r\") as turkish:\n",
        "    for cnt, line in enumerate(turkish):\n",
        "      turkish_list.append(line.rstrip())\n",
        "  \n",
        "  english_list=english_list[:sample]\n",
        "  turkish_list=turkish_list[:sample]\n",
        "  \n",
        "  rtrain = int(sample*0.9)\n",
        "\n",
        "  english_list = tokenization(english_list, t, \"e\")\n",
        "  turkish_list = tokenization(turkish_list, t, \"t\")\n",
        "  english_word = get_words(english_list)\n",
        "  turkish_word = get_words(turkish_list)\n",
        "  english_train, turkish_train = english_list[:rtrain], turkish_list[:rtrain]\n",
        "  english_test, turkish_test = english_list[rtrain:], turkish_list[rtrain:]\n",
        "  print(\"sentences for training have been processed\")\n",
        "  return english_train, turkish_train, english_test, turkish_test, english_word, turkish_word\n",
        "################################################################################\n",
        "\n",
        "\n",
        "############################ perplexity ########################################\n",
        "# The function takes four arguments:\n",
        "#   english_train: English sentences\n",
        "#   turkish_train: Turkish sentences\n",
        "#   t_tables: translation probability table, word-based\n",
        "#   alignments: alignments\n",
        "#\n",
        "# It returns perplexity, which is a float number representing how well the\n",
        "#   probabilities that have been found are suited for the data\n",
        "#   In each iteration of the expectation maximization algorithm, perplexity\n",
        "#   reduces. If it does not change through iterations, this means that \n",
        "#   the algorithm converged.\n",
        "#\n",
        "# This function does not compute the whole perplexity formula, because it is\n",
        "#   computationally inefficient, rather than computing the translation probability\n",
        "#   of a sentence, it looks argmax for each word. Although the computation\n",
        "#   is not same, it gives an idea regarding the convergence of the algorithm\n",
        "################################################################################\n",
        "\n",
        "def perplexity(english_train, turkish_train, t_tables):\n",
        "  s=0\n",
        "  for index_e, e_sentence in enumerate(english_train):\n",
        "    pmax=1\n",
        "    f_sentence = turkish_train[index_e]\n",
        "    le = len(e_sentence)\n",
        "    lf = len(f_sentence)\n",
        "    s+=np.log(max(t_tables[e_word][f_word] \\\n",
        "\t\t\tfor e_word in e_sentence  \\\n",
        "\t\t\tfor f_word in f_sentence))\n",
        "  print(\"perplexity: {}\".format(-s))\n",
        "  return -s\n",
        "  \n",
        "################################################################################\n",
        "\n",
        "###################### expectation_maximization ################################\n",
        "# The function takes english_word, turkish_word, english_train, turkish_train, tablelist, \n",
        "#   english_word: a list of english words in the corpus\n",
        "#   turkish_word: a list of turkish words in the corpus\n",
        "#   english_train: english sentences the model to be trained as a list of list\n",
        "#   turkish_train: turkish sentences the model to be trained as a list of list\n",
        "#   tablelist: is a list of dictionaries created by get_t_table\n",
        "#     [t_table, count_tables, s_totals, total_tables]\n",
        "#\n",
        "# The function returns translation probabilities named t_table\n",
        "################################################################################\n",
        "\n",
        "def expectation_maximization(english_word, turkish_word, english_train, turkish_train, tablelist, op):\n",
        "  [t_tables, count_tables, s_totals, total_tables] = tablelist\n",
        "  count = 1                                      # to count how many steps taken\n",
        "  epsilon = 1                                    # to determine convergence\n",
        "  lastval = 0\n",
        "  while True:\n",
        "    print(\"✖ step {}\".format(count))\n",
        "    start = time.time()\n",
        "    if count > 1:\n",
        "      [count_tables, total_tables] = to_zero(count_tables, total_tables)\n",
        "    for index_e, e_sentence in enumerate(english_train):\n",
        "      f_sentence = turkish_train[index_e]\n",
        "      for e_word in e_sentence:\n",
        "        s_totals[e_word]=0\n",
        "        for f_word in f_sentence:\n",
        "          s_totals[e_word]+=t_tables[e_word][f_word]\n",
        "      for e_word in e_sentence:\n",
        "        for f_word in f_sentence:\n",
        "          count_tables[e_word][f_word]+=(t_tables[e_word][f_word]/s_totals[e_word])\n",
        "          total_tables[f_word]+=(t_tables[e_word][f_word]/s_totals[e_word])\n",
        "    for e_word, val in t_tables.items():\n",
        "      for f_word, value in val.items():\n",
        "        t_tables[e_word][f_word]=(count_tables[e_word][f_word]/total_tables[f_word])\n",
        "    table_rv, t_tables = normalize(t_tables)\n",
        "    s = perplexity(english_train, turkish_train, t_tables)\n",
        "    print(\"difference in perplexity:\", s-lastval)\n",
        "    end = time.time()\n",
        "    print(\"execution time: {}\".format(end-start))\n",
        "    if not count==1:\n",
        "      if abs(lastval- s) < epsilon or s > lastval:\n",
        "        print(\"Expectation Maximization Algorithm is Converged within {} Steps\".format(count))\n",
        "        save_t_table(t_tables, op)\n",
        "        break\n",
        "    lastval = s\n",
        "    count+=1\n",
        "  return t_tables, table_rv\n",
        "  \n",
        "################################################################################\n",
        "\n",
        "\n",
        "################################ prob_al #######################################\n",
        "# The function takes four arguments:\n",
        "#   t_tables, alignments, t_sentence as a Turkish sentence\n",
        "#   e_sentence as an English sentence, op an operator to determine the direction\n",
        "#   of translation.\n",
        "#\n",
        "# The function finds all probable alignments between source and target sentences\n",
        "#   and returns a normalized probability for each of them with a nested dictionary\n",
        "# Additionally, it sums up probabilities of all probable alignments to find\n",
        "#   the translation probability of source and target sentence\n",
        "################################################################################\n",
        "\n",
        "def prob_al(t_tables, t_sentence, e_sentence, op):\n",
        "  t_sentence = tokenization([t_sentence], op, \"t\")[0]\n",
        "  e_sentence = tokenization([e_sentence], op, \"e\")[0]\n",
        "  if op==\"t2e\":         # table is t[e_word][f_word] or t[target][source]\n",
        "    source = t_sentence # determine source and target sentences\n",
        "    target = e_sentence\n",
        "  else:\n",
        "    source = e_sentence\n",
        "    target = t_sentence\n",
        "\n",
        "  print(\"source sentence:\", source)\n",
        "  print(\"target sentence:\", target)\n",
        "\n",
        "  al=[]\n",
        "  for t in [\" \"]+target: # source can be not-aligned\n",
        "    wal=[]\n",
        "    for s in source:\n",
        "      wal.append([s, t]) # probable alignments\n",
        "    al.append(wal)\n",
        "\n",
        "  ali={}\n",
        "  ind=0\n",
        "\n",
        "  for a in product(*al): # find sentence alignments\n",
        "                                   # target should have one alignment\n",
        "    if len(set([l[1] for l in a])) == len(target)+1: \n",
        "      p_a=1\n",
        "      for l in a:\n",
        "        if l[1]!=\" \":\n",
        "          # translation and alignment prob.\n",
        "          if l[1] in t_tables and l[0] in t_tables[l[1]]:\n",
        "            p_a*=t_tables[l[1]][l[0]]\n",
        "            \n",
        "          else:\n",
        "            p_a*=0\n",
        "      if p_a!=0:\n",
        "        ali[ind]=[a, p_a]\n",
        "    ind+=1\n",
        "\n",
        "  p_sum=0\n",
        "  for key in ali:\n",
        "    p_sum+=ali[key][1] # sum over probabilities\n",
        "    \n",
        "  for key in ali:\n",
        "    if p_sum!=0:\n",
        "      ali[key][1]/=p_sum # normalization\n",
        "\n",
        "  values = list(set([ali[key][1] for key in ali]))\n",
        "  values.sort(reverse = True)\n",
        "  \n",
        "  sortedali = {}\n",
        "  for val in values[:2]:\n",
        "      for key in ali:\n",
        "          if ali[key][1] == val:\n",
        "              sortedali[key]=ali[key]\n",
        "           \n",
        "\n",
        "  return sortedali, p_sum\n",
        "################################################################################\n",
        "      \n",
        "\n",
        "################################ prob_word #####################################\n",
        "# The function takes three arguments\n",
        "#   t_table as a t_table\n",
        "#   word as a word in the corpus\n",
        "#   number is the maximum n probable words for word\n",
        "#\n",
        "# The function returns a list of tuples\n",
        "#   such that \n",
        "#   maxtuplist=[(ev, house, 0.5), \n",
        "#               (ev, home, 0.4), \n",
        "#               (ev, book, 0.001) ...]\n",
        "################################################################################\n",
        "\n",
        "def prob_word(t_table, word, n):\n",
        "  tuplist=[]\n",
        "  maxtuplist=[]\n",
        "  for t_word, t_values in t_tafble.items():\n",
        "    for s_word, s_values in t_values.items():\n",
        "      if word == s_word:\n",
        "        tuplist.append((s_word, t_word, s_values))\n",
        "  values=sorted([i[2] for i in tuplist])[:n-1]\n",
        "  for val in values:\n",
        "    for tup in tuplist:\n",
        "      if tup[2]==val:\n",
        "        maxtuplist.append(tup)\n",
        "  return maxtuplist\n",
        "################################################################################\n",
        "\n",
        "def table_rv(t_tables):\n",
        "  rv_table={}\n",
        "  for key, val in t_tables.items():\n",
        "    for k2, v2 in val.items():\n",
        "      if k2 not in rv_table:\n",
        "        rv_table[k2]={}\n",
        "      rv_table[k2][key]=v2\n",
        "  return rv_table\n",
        "\n",
        "def normalize(t_table):\n",
        "  rv_table = table_rv(t_table)\n",
        "  for f, e in rv_table.items():\n",
        "    n = sum(list(e.values()))\n",
        "    for ew in e:\n",
        "      e[ew]/=n\n",
        "  for e, f in t_table.items():\n",
        "    for f_word in f:\n",
        "      t_table[e][f_word]=rv_table[f_word][e]\n",
        "  return rv_table, t_table\n",
        "\n",
        "def train(n_sample, d, t):\n",
        "  english_train, turkish_train, english_test, turkish_test, english_word, turkish_word = get_sample(n_sample, t)\n",
        "  print(\"Number of English Sentence {}, Turkish Sentence {}\".format(len(english_train), len(turkish_train)))\n",
        "  print(\"Number of English Word {}, Turkish Word {}\".format(len(english_word), len(turkish_word)))\n",
        "  if t == \"t2e\":\n",
        "    tablelist = get_t_table(english_train, turkish_train, turkish_word)\n",
        "    t_tables, table_rv = expectation_maximization(english_word, turkish_word, english_train, turkish_train, tablelist, d)\n",
        "  elif t == \"e2t\":\n",
        "    tablelist = get_t_table(turkish_train, english_train, english_word)\n",
        "    t_tables, table_rv = expectation_maximization(turkish_word, english_word, turkish_train, english_train, tablelist, d)\n",
        "  return t_tables, table_rv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXEU-cRTj6VE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_sentence = \"Okula giderim\"\n",
        "e_sentence = \"I go to the school\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}